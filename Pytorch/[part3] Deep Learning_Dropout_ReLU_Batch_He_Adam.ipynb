{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-insertion",
   "metadata": {},
   "source": [
    "# 1. MLP 설계할 떄 dropout 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-minister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.0+cpu  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # 배치 사이즈 32\n",
    "EPOCHS = 10 # 10번의 에폭 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "martial-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdqj\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incredible-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cross-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5 # Dropout 생성\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x) # 시그모이드 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob) # Dropout 생성\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob) # Dropout 생성\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compatible-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # 학습 SGD(momentum), lr = 0.01 지정\n",
    "criterion = nn.CrossEntropyLoss() # binary 이니까 crossEntropyLoss\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    \n",
    "    # 확인하는 구간 정의\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "several-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # no_grad로 초기화, 검증데이터 확인\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "retained-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdqj\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.372763\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.270722\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.347056\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.392092\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.347405\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.285298\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.355855\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.281067\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.327244\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.275053\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 2.2809, \tTest Accuracy: 11.04 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.302710\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.303772\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.282429\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.254463\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.214751\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 2.282789\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 2.282966\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 2.268290\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 2.200683\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 2.051129\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 2.0279, \tTest Accuracy: 35.89 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 2.041611\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.927809\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 1.953266\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.890899\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 1.870854\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.579884\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 1.648741\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 1.683738\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 1.610824\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 1.598120\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2218, \tTest Accuracy: 62.27 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1.117579\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 1.459902\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 1.087082\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 1.175145\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 1.229296\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 1.136676\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 1.168850\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 1.068970\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 1.161522\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 1.197580\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.8738, \tTest Accuracy: 70.37 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 1.040784\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 1.386800\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 1.152333\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.855301\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.848849\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 1.025663\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.859096\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 1.110849\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 1.291453\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 1.128986\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.7513, \tTest Accuracy: 76.33 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.573357\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.676559\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.679619\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.741007\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.801949\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.839711\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 1.042728\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.562524\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.870347\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.755204\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.6502, \tTest Accuracy: 80.46 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.606973\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.828473\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.880000\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.705052\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.993547\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.778238\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.642323\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.539159\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.622237\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 1.365205\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.5673, \tTest Accuracy: 83.01 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.527433\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.687990\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.957794\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.478088\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.662212\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.669000\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.743116\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.600499\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.678999\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.713560\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.5091, \tTest Accuracy: 85.11 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.822904\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.551839\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.721901\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.692659\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.544858\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.798285\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.527098\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.540408\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.568262\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.619040\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.4693, \tTest Accuracy: 86.05 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.930008\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.474966\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.570176\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.592524\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.929003\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.665854\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.375906\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.795352\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.456211\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.505559\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.4387, \tTest Accuracy: 86.92 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # 모델 학습\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    # 테스트 검증\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-verse",
   "metadata": {},
   "source": [
    "# 2. ReLU 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amazing-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)       # sigmoid 대신 ReLU가 들어갔다!!\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)       # sigmoid 대신 ReLU가 들어갔다!!\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "another-license",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdqj\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.413355\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.258033\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.402081\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.284401\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.244243\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.324172\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.236053\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.273867\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.237538\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.265991\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 2.2818, \tTest Accuracy: 18.07 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.318339\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.320609\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.337325\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.240350\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.292237\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 2.208195\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 2.200290\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 2.222617\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 2.070103\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 2.137913\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 2.0604, \tTest Accuracy: 38.65 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 2.098846\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.996731\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 2.109353\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.843818\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 1.702869\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.825806\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 1.672153\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 1.459805\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 1.495227\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 1.575033\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2545, \tTest Accuracy: 59.61 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1.242110\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 1.428775\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 1.227596\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 1.164057\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 1.362753\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 1.099102\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 1.219028\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 1.259418\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.985549\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 1.025097\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.8974, \tTest Accuracy: 70.74 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.965020\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 1.091116\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.759525\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.749980\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 1.195926\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.925051\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.958672\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.729294\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 1.005616\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 1.091275\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.7584, \tTest Accuracy: 76.18 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.618673\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.969363\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.849724\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.914865\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 1.012866\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.786233\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 1.105878\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.821631\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.866923\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.883392\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.6499, \tTest Accuracy: 80.84 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.892566\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.676404\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.770386\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.587729\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.961347\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.644889\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.486319\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.738987\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 1.059354\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.642219\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.5620, \tTest Accuracy: 83.54 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.677125\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.771078\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.944890\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.525620\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 1.037069\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.646290\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.648583\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.810050\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.697752\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.761979\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.5039, \tTest Accuracy: 85.24 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.526309\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.400909\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.591608\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.561497\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.528521\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.695940\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.577206\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.557441\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.426120\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 1.027708\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.4691, \tTest Accuracy: 85.89 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.676811\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.769367\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.752328\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.552511\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.660396\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.790921\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.438177\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.452892\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.514211\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.740151\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.4395, \tTest Accuracy: 86.97 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-generation",
   "metadata": {},
   "source": [
    "# 3. Batch Normalization 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "israeli-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 0.739340\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.307793\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.473834\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.300209\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.774276\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.530995\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.779925\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.464488\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.547864\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.499111\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.4206, \tTest Accuracy: 87.58 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.659648\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.536110\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 1.092418\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.374606\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.386496\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.654428\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.374927\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.431464\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.257174\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.443716\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.4055, \tTest Accuracy: 87.82 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.538613\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.485804\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.387665\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.482044\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.447384\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.551820\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.345198\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.358045\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.312694\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.659768\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.3917, \tTest Accuracy: 88.29 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.943747\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.365237\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.495717\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.406069\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.785020\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.677892\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.665591\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.419353\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.355746\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.520612\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.3777, \tTest Accuracy: 88.79 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.850167\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.593567\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.480062\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.621820\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.391033\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.484738\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.458781\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.454654\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.432362\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.309393\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.3679, \tTest Accuracy: 88.98 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.263113\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.296214\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.524890\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.632164\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.555899\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.406085\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.420790\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.471889\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.376577\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.248318\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.3575, \tTest Accuracy: 89.33 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.359601\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.270766\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.548614\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.383756\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.439803\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.289067\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.633938\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.397695\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.234386\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 1.001683\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.3482, \tTest Accuracy: 89.57 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.137656\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.638906\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.537366\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.654760\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.617226\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.463062\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.330373\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.396139\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.369078\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.495142\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.3404, \tTest Accuracy: 89.87 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.721783\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.306897\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.324350\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.433893\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.434011\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.718967\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.341905\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.227669\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.542609\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.282673\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.3327, \tTest Accuracy: 90.05 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.542539\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.252737\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.244769\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.200443\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.323558\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.448621\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.285651\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.274412\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.531576\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.530839\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.3254, \tTest Accuracy: 90.26 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-health",
   "metadata": {},
   "source": [
    "# 4. He Initialization 초기화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "productive-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data) # kaiming_uniform_는 He 초깃값이다.\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vital-beginning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.539637\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.650326\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.528997\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.491456\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.382745\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 1.023613\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.575118\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.476374\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.642743\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.254190\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.2224, \tTest Accuracy: 93.29 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.396919\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.254240\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.412769\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.370537\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.179562\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.428764\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.437366\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.262507\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.242531\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.421704\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1709, \tTest Accuracy: 94.82 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.410118\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.334447\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.353374\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.405983\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.294845\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.337234\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.445749\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.464165\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.474560\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.211276\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.1496, \tTest Accuracy: 95.38 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.255073\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.091589\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.120896\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.083892\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.348748\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.375552\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.101138\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.462222\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.427986\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.104486\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.1286, \tTest Accuracy: 96.03 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.511714\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.645228\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.354189\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.747811\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.323182\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.164702\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.132836\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.510033\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.179001\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.112143\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.1188, \tTest Accuracy: 96.45 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.587155\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.101952\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.485101\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.112788\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.171755\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.586203\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.170435\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.340611\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.048358\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.165244\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.1088, \tTest Accuracy: 96.74 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.194589\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.093158\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.276997\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.299633\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.165871\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.248073\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.208284\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.242026\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.654683\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.113814\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.1052, \tTest Accuracy: 96.63 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.205732\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.545570\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.382371\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.114878\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.466508\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.144810\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.110433\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.218916\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.244114\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.023212\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0974, \tTest Accuracy: 96.95 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.155068\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.245094\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.494640\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.190117\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.151429\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.283869\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.044512\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.065022\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.156251\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.105403\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0906, \tTest Accuracy: 97.13 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.207021\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.255109\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.187903\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.174631\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.028402\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.992312\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.219358\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.044070\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.188543\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.231341\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0898, \tTest Accuracy: 97.25 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-associate",
   "metadata": {},
   "source": [
    "# 5. Optimizer 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intense-comfort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data) # kaiming_uniform_는 He 초깃값이다.\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "incomplete-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.665775\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.681709\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.209609\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.134814\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.511366\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.533447\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.386680\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.246109\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.219392\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.434873\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.1246, \tTest Accuracy: 96.01 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.482386\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.257432\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.324007\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.118995\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.212950\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.143802\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.358522\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.378411\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.478928\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.066714\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1065, \tTest Accuracy: 96.66 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.269735\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.175546\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.175976\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.105806\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.334514\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.619786\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.370492\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.139021\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.262664\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.176657\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0954, \tTest Accuracy: 97.14 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.070155\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.098638\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.125455\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.258546\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.121128\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.563210\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.333456\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.039214\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.174407\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.228234\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0822, \tTest Accuracy: 97.32 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.124502\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.649067\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.166774\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.165961\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.106538\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.138829\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.521354\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.055217\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.162636\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.075347\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0785, \tTest Accuracy: 97.63 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.125766\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.057687\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.018609\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.214236\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.205741\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.161332\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.152447\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.156149\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.268385\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.233646\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0719, \tTest Accuracy: 97.83 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.030122\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.045199\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.127157\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.222642\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.239778\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.188947\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.038965\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.235858\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.088004\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.087944\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0769, \tTest Accuracy: 97.54 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.138566\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.116123\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.149617\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.176440\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.055576\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.102326\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.070521\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.066415\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.183994\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.296849\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0679, \tTest Accuracy: 97.89 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.099997\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.202304\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.062524\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.226069\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.111040\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.082137\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.192666\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.162891\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.034538\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.152004\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0685, \tTest Accuracy: 97.82 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.284974\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.055409\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.228676\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.107270\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.217953\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.039647\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.040420\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.241915\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.014091\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.108865\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0668, \tTest Accuracy: 97.86 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
