{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "encouraging-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wound-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.0+cpu  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assumed-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치와 에폭 설정\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "union-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdqj\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,          # train용\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,          # test용 \n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intermediate-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "    \n",
    "    # 32의 배치사이즈, 1개의 데이터, 28, 28 크기의 데이터\n",
    "    # 레이블은 32개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' 5. 데이터 확인하기 (2) '''\n",
    "# pltsize = 1\n",
    "# plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(1, 10, i + 1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "#     plt.title('Class: ' + str(y_train[i].item()))\n",
    "\n",
    "## 로컬에서 실행 시 커널이 죽어버림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forward-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # fully connected, 28 * 28은 512개의 노드\n",
    "        self.fc2 = nn.Linear(512, 256)     # 512개의 노드를 256개로\n",
    "        self.fc3 = nn.Linear(256, 10)      # 최종 아웃풋은 10개의 레이블\n",
    "\n",
    "    def forward(self, x):                  # 순전파 feed forward\n",
    "        x = x.view(-1, 28 * 28)            # 2차원을 1차원으로, 이를 Flatten 이라함\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)      # 최종 활성함수는 softmax인데, log.softmax\n",
    "        return x                          # log.softmax는 좀 더 원활하게 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "selective-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)         \n",
    "\n",
    "# optim 최적화, 즉 학습관련 기술은 SGD, momentum, Adagrad 등 있음\n",
    "# 나중에 다시 한번 공부하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "final-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()                  # 최적화 초기화 \n",
    "        output = model(image)                  # image 데이터를 통해 output\n",
    "        loss = criterion(output, label)        # output과 label 비교가 loss\n",
    "        loss.backward()                        # 역전파\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:     # 모니터링하는 구간\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "august-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()           # 평가상태\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]  # 가장 큰 값의 index\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() # 정답갯수\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)      # 배치사이즈 별 loss\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vertical-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdqj\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.296040\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.323861\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.277985\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.325269\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.276271\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.277673\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.275075\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.257281\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.210144\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.277301\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 2.2278, \tTest Accuracy: 47.52 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.225406\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.218935\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.162844\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.087265\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 1.935681\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 1.903102\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 1.625449\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 1.529364\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 1.436831\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 1.408369\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.2265, \tTest Accuracy: 64.92 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 1.287990\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.368640\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.873138\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.972298\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.973518\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.935367\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.969473\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.902337\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.720201\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.551034\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.7352, \tTest Accuracy: 78.31 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.650740\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.657707\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.575008\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.689526\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.829688\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.497195\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.685820\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.721187\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.477704\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.630263\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.5443, \tTest Accuracy: 84.17 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.627794\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.475409\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.323758\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.522173\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.587614\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.443889\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.275701\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.656041\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.560621\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.606057\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.4523, \tTest Accuracy: 87.07 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.721916\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.455627\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.308251\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.526297\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.210850\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.482529\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.264472\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.414302\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.348843\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.426958\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.4077, \tTest Accuracy: 88.08 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.294659\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.591685\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.208091\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.700554\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.582385\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.523844\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.509306\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.183264\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.586836\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.311039\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.3809, \tTest Accuracy: 89.04 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.229194\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.248698\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.266489\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.184123\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.175034\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.413031\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.302463\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.374577\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.651207\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.473071\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.3648, \tTest Accuracy: 89.29 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.331228\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.292884\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.361265\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.278147\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.208867\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.483779\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.361738\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.348374\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.379950\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.262313\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.3498, \tTest Accuracy: 89.80 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.315992\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.449891\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.163763\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.394815\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.178737\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.453382\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.255425\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.259100\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.275305\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.344715\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.3359, \tTest Accuracy: 90.22 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200) \n",
    "    # MLP모델, 학습데이터, optimizer - SGD\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
